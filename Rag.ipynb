{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Tutorial: Building a RAG System with Groq and HuggingFace\n",
                "\n",
                "This notebook demonstrates how to build a RAG pipeline using LangChain, **Groq**, and **HuggingFace**.\n",
                "\n",
                "## 0. Setup and Environment Check\n",
                "\n",
                "We define a helper function `check_env()` to ensure our API key is ready."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\Tomas\\AppData\\Roaming\\Python\\Python314\\site-packages\\langchain_core\\_api\\deprecation.py:25: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
                        "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
                        "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "\n",
                "from dotenv import load_dotenv\n",
                "from langchain_community.document_loaders import WebBaseLoader\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
                "from langchain_groq import ChatGroq\n",
                "from langchain_huggingface import HuggingFaceEmbeddings\n",
                "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "from langchain_core.vectorstores import InMemoryVectorStore\n",
                "\n",
                "load_dotenv()\n",
                "\n",
                "def check_env():\n",
                "    \"\"\"Verify that required environment variables are set.\"\"\"\n",
                "    if not os.getenv(\"GROQ_API_KEY\"):\n",
                "        print(\"\\n[!] ERROR: GROQ_API_KEY not found in .env or environment.\")\n",
                "        print(\"Please create a .env file with your key: GROQ_API_KEY=gsk_...\\n\")\n",
                "        return False\n",
                "    return True\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Documents\n",
                "\n",
                "We load content from the web."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_documents(urls: list[str]):\n",
                "    \"\"\"Load web pages and return a list of LangChain Document objects.\"\"\"\n",
                "    loader = WebBaseLoader(urls)\n",
                "    docs = loader.load()\n",
                "    print(f\"[load] Loaded {len(docs)} document(s).\")\n",
                "    return docs"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Split into Chunks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def split_documents(docs, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
                "    \"\"\"Split documents into smaller chunks for embedding.\"\"\"\n",
                "    splitter = RecursiveCharacterTextSplitter(\n",
                "        chunk_size=chunk_size,\n",
                "        chunk_overlap=chunk_overlap,\n",
                "    )\n",
                "    splits = splitter.split_documents(docs)\n",
                "    print(f\"[split] Created {len(splits)} chunk(s).\")\n",
                "    return splits"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Embed & Store (HuggingFace)\n",
                "\n",
                "We use **HuggingFace** for free embeddings and an in-memory store."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_vectorstore(splits):\n",
                "    \"\"\"Embed chunks and store them in an in-memory vector store using HuggingFace.\"\"\"\n",
                "    # This uses a free embedding model from HuggingFace\n",
                "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
                "    vectorstore = InMemoryVectorStore.from_documents(\n",
                "        documents=splits,\n",
                "        embedding=embeddings,\n",
                "    )\n",
                "    print(\"[store] Vector store built successfully (HuggingFace In-Memory).\")\n",
                "    return vectorstore\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Build RAG Chain (Groq)\n",
                "\n",
                "We use **ChatGroq** with the Llama-3.3 model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_rag_chain(vectorstore):\n",
                "    \"\"\"\n",
                "    Build a RAG chain that:\n",
                "    - Retrieves the top-k most relevant chunks.\n",
                "    - Formats them into a prompt.\n",
                "    - Generates an answer with an OpenAI LLM.\n",
                "    \"\"\"\n",
                "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
                "\n",
                "    prompt = ChatPromptTemplate.from_messages(\n",
                "        [\n",
                "            (\n",
                "                \"system\",\n",
                "                (\n",
                "                    \"You are an assistant for question-answering tasks. \"\n",
                "                    \"Use the following pieces of retrieved context to answer \"\n",
                "                    \"the question. If you don't know the answer, say that you \"\n",
                "                    \"don't know. Use three sentences maximum and keep the \"\n",
                "                    \"answer concise.\\n\\nContext:\\n{context}\"\n",
                "                ),\n",
                "            ),\n",
                "            (\"human\", \"{question}\"),\n",
                "        ]\n",
                "    )\n",
                "\n",
                "    llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
                "\n",
                "    def format_docs(docs):\n",
                "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
                "\n",
                "    rag_chain = (\n",
                "        RunnableParallel(\n",
                "            context=retriever | format_docs,\n",
                "            question=RunnablePassthrough(),\n",
                "        )\n",
                "        | prompt\n",
                "        | llm\n",
                "        | StrOutputParser()\n",
                "    )\n",
                "\n",
                "    return rag_chain\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Usage\n",
                "\n",
                "Run the cell below to ask your question."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[load] Loaded 1 document(s).\n",
                        "[split] Created 66 chunk(s).\n",
                        "[store] Vector store built successfully (HuggingFace In-Memory).\n",
                        "\n",
                        "=== RAG Question-Answering Session ===\n",
                        "Type 'exit' to quit.\n",
                        "\n",
                        "Goodbye!\n"
                    ]
                }
            ],
            "source": [
                "def main():\n",
                "    if not check_env():\n",
                "        return\n",
                "\n",
                "    # Source URLs â€“ feel free to replace with your own documents.\n",
                "    urls = [\n",
                "        \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
                "    ]\n",
                "\n",
                "    docs = load_documents(urls)\n",
                "    splits = split_documents(docs)\n",
                "    vectorstore = build_vectorstore(splits)\n",
                "    rag_chain = build_rag_chain(vectorstore)\n",
                "\n",
                "    print(\"\\n=== RAG Question-Answering Session ===\")\n",
                "    print(\"Type 'exit' to quit.\\n\")\n",
                "\n",
                "    while True:\n",
                "        question = input(\"Your question: \").strip()\n",
                "        if question.lower() in {\"exit\", \"quit\", \"q\"}:\n",
                "            print(\"Goodbye!\")\n",
                "            break\n",
                "        if not question:\n",
                "            continue\n",
                "        answer = rag_chain.invoke(question)\n",
                "        print(f\"\\nAnswer: {answer}\\n\")\n",
                "\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
